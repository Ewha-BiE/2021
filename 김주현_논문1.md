## 논문 1: Can Questions Summarize A Corpus?  
### Using Question Generation for Characterizing COVID-19 Research
#### Gabriela Surita, Rodrigo Nogueira, and Roberto Lotufo 
(Sep 22, 2020)


```python
import os
from datetime import datetime, timedelta

import torch 
import pandas as pd
from tqdm.notebook import tqdm

tqdm.pandas()

device = 'cuda' if torch.cuda.is_available() else 'cpu'

os.cpu_count(), torch.cuda.get_device_name() if device=='cuda' else None
```




    (8, None)




```python
import wget
url = 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-08-26.tar.gz'
wget.download(url)
```


```python
! tar -xf cord-19_2020-08-26.tar.gz
! tar -xf 2020-08-26/document_parses.tar.gz
```


```python
import re
import json 
import itertools
from pathlib import Path

import pandas as pd

path = Path('./document_parses/pdf_json')
files = list(path.iterdir())

print('Files:', len(files))

def preprocessing(text):
    # remove mail
    text = re.sub(r'[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}', 'MAIL', text)
    # remove https
    text = re.sub(r'https\:\/\/doi\.org[^\s]+', 'DOI', text)
    # remove single character for spacing error (최소 3회 반복된 경우)
    text = re.sub(r'(\w\s+){3,}', ' ', text)
    # tag ([] [] []) --> whitespace
    text = re.sub(r'(\[\d+\]\, ?\s?){3,}(\.|\,)?', '\g<2>', text)
    # tag ([ , , ]) --> whitespace
    text = re.sub(r'\[[\d\, \s]+\]', ' ', text)
    # 
    text = re.sub(r'(\(\d+\)\s){3,}', ' ', text)
    # '.' --> ','
    text = re.sub(r'(\d)\.(\d+)', '\g<1>,\g<2>', text)
    # remove all full stops as abbreviations (e.g, i.e, cit, ...)
    text = re.sub(r'\.(\s)?([^A-Z\s])', '\g<1>\g<2>', text)
    # correctly spacing the tokens
    text = re.sub(r' {2,}', ' ', text)
    text = re.sub(r'\.{2,}', ' ', text)
    # return lowercase text
    return text.lower()

def read_paper(p):
    """Read one paper to the dataframe."""
    
    with open(p) as f:
        data = json.load(f)
        id = data['paper_id']
        title = data['metadata']['title']
        text_blocks = (preprocessing(p['text']) for p in data['abstract'] + data['body_text'])
        
        return {'text':' '.join(text_blocks), 'title': title, 'paper_id': id}
    
    paper_df = pd.DataFrame(read_paper(p) for p in tqdm(files))
```


```python
# Merges the metadata with the actual dataset.

metadata = pd.read_csv('2020-08-26/metadata.csv')

merged_df = pd.merge(metadata, paper_df, left_on='sha', right_on='paper_id')
merged_df['publish_time'] = pd.to_datetime(merged_df['publish_time'])
```


```python
# Filter only papers related COVID-19
# https://www.kaggle.com/matteomuffo/a-fine-grained-covid-19-question-answering-engine

covid_terms = ['covid', 'coronavirus disease 19', 'sars cov 2', '2019 ncov',
               '2019ncov', '2019 n cov', '2019n cov', 'ncov 2019', 'n cov 2019',
               'coronavirus 2019', 'wuhan pneumonia', 'wuhan virus', 'wuhan coronavirus',
               'coronavirus 2', 'covid-19', 'SARS-CoV-2', '2019-nCov']
covid_terms = [elem.lower() for elem in covid_terms]
covid_terms = re.compile('|'.join(covid_terms))
            ## compile 미리 해두고 이를 저장하여 반복문에서 사용

merged_df['is_covid'] = (
    merged_df.text.progress_apply(lambda r: bool(covid_terms.search(r)))
    & (merged_df.publish_time > datetime(2019, 10, 1))
)

covid_df = merged_df[merged_df.is_covid]
```


```python
# Save the results
covid_df.to_csv('./covid.csv')
```

### Question Generation  


```python
covid_df = pd.read_csv('./covid.csv')
```


```python
! wget -nc https://storage.googleapis.com/doctttttquery_git/t5-base.zip
! unzip -o t5-base.zip
```


```python
import torch
from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration

# instantiate and load the QG model to the GPU
qg_tokenizer = T5Tokenizer.from_pretrained('t5-base')
qg_config = T5Config.from_pretrained('t5-base')
qg_model = T5ForConditionalGeneration.from_pretrained('model.ckpt-1004000', from_tf=True, config=qg_config)

qg_model.to(device)

True
```


```python
import nltk
from nltk.tokenize import sent_tokenize

nltk.download('punkt')

def generate_questions_doc_t5_query(context, q=1):
    """
    Generate q questions for the provided text context.
    """
    doc_text = f"{context} </s>"
    
    with torch.no_grad():
        input_ids = qg_tokenizer.encode(doc_text, return_tensors='pt').to(device)
        outputs = qg_model.generate(
            input_ids = input_ids,
            max_length = 64,
            do_sample = False,
            num_return_sequences = q
        )
        return [
            qg_tokenizer.decode(output)
            for output in outputs
        ]
    
    def batch_doc(text, span=10, stride=5):
        """
        Convert a document into a list of chunks with 10 span of 10 and strie of 5 sentences.
        """
        sentences = sent_tokenize(text)
        chunks = [' '.join(sentences[i:i+span]) for i in range(0, len(sentences), stride)]
        return chunks
```


```python
# Compute the batches for all dataset
batches = [batch_doc(text) for text in tqdm(covid_df.text)]
```


```python
import numpy as np
# to avoid ill parsed documnets to take very long on the generation step.

max_batch, max_block = (
    np.percentile([len(b) for b in batches], 99),
    np.percentile([len(t) for b in batches for t in b], 99)
)

max_batch, max_block
```


```python
# checkpoint
chkp = 0

# freq to generate a chkp
chkp_freq = 1000

# version of the generation
version = 1

if not chkp:
    question_df = covid_df.copy()
    question_df['questions'] = ''
else:
    question_df = pd.read_csv(f'./cord_questions/covid_questions.{verion}.{chkp}.csv', low_memory=False)
    
def run_generation_step(idx, batch):
    qs = [generate_questinos_doc_t5_query(t[:max_block]) for t in batch[:max_batch]]
    qs_text = '. '.join([q for doc in qs for q in doc])
    question_df.questions.iloc[idx] = qs_text
    
    if idx and not idx % 1000:
        question_df.to_csv(f'./cord_questions/covid_questions.{version}.{idx}.csv')
        
        return qs
    
    questions = [
        run_generation_step(idx, batch)
        for idx, batch in zip(range(chkp, len(covid_df)), tqdm(batches[chkp:]))
    ]
    
    question_df.to_csv(f'./cord_questions/covid_questions.{version}.{len(covid_df)}.csv')
    
```

### Qualitative Experiments


```python
import pands as pd

question_df = pd.read_csv('./cord_questions/covid_questions.1.41529.csv', low_memory=False)
question_df = question_df[~question_df.questions.isna()]
question_df['publish_time_dt'] = pd.to_datetime(question_df.publish_time)

question_df
```


```python
from datetime import datetime

ignore = {
    'preprint',
    'cc-by',
    'copyrigth',
    'can you reuse a file',
    'reserved',
    'medrxiv'
}

question_df = pd.DataFrame(
    dict(
        counts=1,
        date=row.publish_time_dt,
        question=q,
        cord_uid=row.cord_uid
    )
    for row in question_df.iteruples()
    for q in row.questions.split('. ')
    if all(i not in q for i in ignore)
)

question_to_date = question_df[question_df.date < datetime(2020, 8, 27)]
```


```python
# top 15

question_df.groupby('question').sum().counts.sort_values(ascending=False).head(15).reset_index()
```


```python
# unique question

question_df.groupby('question').sum().counts.sort_values(ascending=False).count()
```


```python
import matplotlib
import seaborn as sns
from matplotlib import pyplot as plt

sns.set_style('darkgrid')
matplotlib.rcParams['figure.figsize'] = 8, 6

question_date_group = question_to_date \
    .groupby([question_to_date.date, question_to_date.question]) \
    .sum() \
    .reset_index()

def seq_format(qs):
    def to_group(q):
        group = question_date_group[question_date_group.question.str.contains(q)].groupby('date').sum().rolling(15).mean().reset_index()
        group['term'] = q
        return group
    return pd.concat([
        to_group(q)
        for q in qs
    ])

def plot_terms(ts):
    ax = sns.lineplot(
        x='date'
        y='counts',
        hue='term',
        data=seq_format(tx)
    )
    ax.tick_params(axis='x', rotation=45)
    return ax

plot_terms(['incubation period', 'treatment', 'vaccine'])
#plt.savefig('results/treatments.pdf')
```


```python
# Save top 1000k to csv
question_df.groupby('question').sum().counts.sort_values(ascending=False).head(1000).reset_index().to_csv('./results/top_10k.csv')
```

### Quantitative Experiments


```python
!wget https://raw.githubusercontent.com/castorini/pygaggle/master/data/kaggle-lit-review-0.2.json
```


```python
import json

covid_qa = json.load(open('kaggle-lit-review-0.2.json'))

covid_qa_questions = [
    subcat['kq_name']
    for cat in covid_qa['categories']
    for subcat in cat['sub_categories']
]

covid_qa_articles = [
    [ans['id'] for ans in subcat['answers']]
    for cat in covid_qa['categories']
    for subcat in cat['sub_categories']
]

len(covid_qa_questions)
```


```python
most_frequent = question_df.groupby('question').sum().counts.sort_values(ascending=False).reset_index()
most_frequent = most_frequent[most_frequent.counts >= 3]
```


```python
from tqdm.notebook import tqdm
from bert_score import score, plot_example

most_frequent_list = most_frequent.question.tolist()
results = []

for ref in tqdm(covid_qa_questions):
    candidates = most_frequent_list
    references = [ref for _ in candidates]
    
    P, R, F1 = score(references, candidates, model_type='scibert-scivocab-uncased', lang='en')
    top_3_candidates = sorted(((p, s) for p, s in zip(F1, candidates)), reverse=True)[:3]
    
    for i, (p, s) in enumerate(top_3_candidates):
        results.append({
            'reference': ref,
            'candidate': s,
            'index' : i + 1
        })
        
results_df = pd.DataFrame(results)
results_df.to_csv('results/aggregated_experiment.csv')

results_df
```


```python
results_doc = []

for ref, docs in zip(tqdm(covid_qa_questions), covid_qa_articles):
    for doc in docs:
        doc_matches = question_df[question_df.cord_uid == doc]
        if not len(doc_matches):
            continue
        
        candidates = doc_matches.question.iloc[0].split('. ')
        P, R, F1 = score([ref for _ in candidates], candidates, model_type='scibert-scivocab-uncased', lang='en')
        for i, (_, s) in enumerate(sorted(((p, s) for p, s in zip(F1, candidates)), reverse=True[:3])):
            results_doc.append({
                'reference' : ref,
                'doc' : doc,
                'candidate' : s,
                'index' : i
            })
            
    results_doc_df = pd.DataFrame(results_doc)
    results_doc_df.to_csv('results/per_document_experiment.csv')
    
    results_doc_df
```

### Comparison with LDA


```python
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

no_features = 10000

tf_vectorizer = CountVectorizer(min_df=2, max_features=no_feautures, stop_words='english', ngram_range=[1, 2])
tf = tf_vectorizer.fit_transform(covid_df.text)

lda = LatentDirichletAllocation(n_components=20)
lda.fit(tf)
```


```python
feature_names = tf_vectorizer.get_feauture_names()

for topic_idx, topic in enumerate(lda.components_):
    print(' '.join([feature_names[i].replace(' ', '_') for i in topic.argsort()[:-6:-1]]))
```

### Comparison with Word Clouds


```python
counts = tf.toarray().sum(axis=0)
words = [k for k, v in sorted(tf_vectorizer.vocabulary_.items(), key=lambda v: v[1])]
frequencies = {w: c for w, c in zip(words, counts)}
```


```python
wc = dict(sorted(frequencies.items(), key=lambda v: -v[1])[:500])
```


```python
from matplotlib import pyplot as plt
from wordcloud import WordCloud

wordcloud = WordCloud(background_color='white')
wordcloud.generate_from_frequencies(frequencies=wc)

plt.figure(figsize=(9, 5))
plt.imshow(wordcloud)
plt.axis('off')
plt.savefig('results/wordcloud.pdf')
plt.show();
```

### Combining questions with LDA


```python
no_features = 10000

q_vectorizer = CountVectorizer(min_df=2, max_features=no_features, stop_words='english', ngram_range=[1, 2])
qtf = q_vectorizer.fit_transform(most_frequent.question)

ldaq = LatentDirichletAllocation(n_components=20)
ldaq.fit(qtf)
```


```python
probs = ldaq.transform(qtf)
probs
```


```python
feature_names = q_vectorizer.get_feauture_names()

for topic_idx, topic in enumerate(ldaq.components_):
    print(' '.join([feature_names[i].replace(' ', '_') for i in topic.argsort()[:-6:-1]]))
```


```python
pd.set_option('display.max_colwidth', 100)
rel = probs * np.tile(most_frequent.counts.to_numpy().reshape(-1, 1), 20)
most_frequent.iloc[rel.argmax(axis=0)]
```


```python
for q in most_frequent.iloc[rel.argmax(axis=0)].question.tolist():
    print(q)
```
