# 잡아라! 텍스트 마이닝 with 파이썬 (1)

출판사 : BJpublic

지은이 : 서대호

## 텍스트 데이터

### 정규 표현식

- 괄호로 둘러싸인 이메일 주소를 삭제하기 위한 정규 표현식

```python
import re

string = '파이썬 기자(python@google.com)'
re.sub('\([A-Za-z0-9\._+]+@[A-Za-z]+\.(com|org|edu|net|co,kr)\)','', string)

# \([A-Za-z0-9\._+]+ --> 이메일 주소가 괄호에 둘러싸여 있으므로 escape 문자 '\'와 '(' 먼저 입력
									## --> 대괄호 [] 안에 이메일 주소 패턴 삽입. 즉, 알파벳의 각 대소문자, 0부터 9까지 숫자, 마침표, _ 기호, + 기호 중 하나라도 포함되어 있는 경우 모두 찾아줌
									## --> 대괄호 뒤 +는 바로 앞에 있는 패턴이 최소 한 번 이상 나타나야 한다는 의미
# @ --> 이메일 주소 다음에는 반드시 @ 표시 필요
# [A-Za-z]+ --> @ 다음에 나올 도메인 주소 형식 표현
# (com|org|edu|net|co,kr)\) --> 도메인 주소 마침표 뒤에 나올 패턴 나열 & 마지막에 escape 문자와 ')'를 입력하여 이메일 주소를 닫는 괄호 표현
```

- 기타 정규 표현식

'`*`' 바로 앞에 있는 문자, 하위 표현식이 0번 이상 반복

'`+`' 바로 앞에 있는 문자, 하위 표현식이 1번 이상 반복

'`[]`' 대괄호 안에 있는 문자 중 하나가 나타남

'`()`' 괄호 안의 정규식을 하위 표현식 그룹으로 만듦 (정규 표현식을 평가할 때 하위 표현식을 가장 먼저 평가)

'`.`' 형태에 상관없이 문자 한 글자를 나타냄

'`^`' 바로 뒤에 있는 문자, 하위 표현식이 문자열 맨 앞에 나타남

'`$`' 바로 앞에 있는 문자, 하위 표현식이 문자열 맨 뒤에 나타남

'`{m}`' 바로 앞에 있는 문자, 하위 표현식이 m번 반복

'`{m, n}`' 바로 앞에 있는 문자, 하위 표현식이 m번 이상, n번 이하로 나타남

'`|`' | 로 분리된 문자, 문자열, 하위 표현식 중 하나가 나타남

'`[^]`' 대괄호 안에 있는 문자를 제외한 문자가 나타남

- 정규 표현식 예제

1) 'a'가 한 번 이상 나오고 'b'가 0번 이상 나오는 문자열 찾는 코드

```python
import re
r = re.compile('a+b*')
r.findall('aaaa, cc, bbbb, aabbb')
# 결과 : ['aaaa', 'aabbb']
```

2) 대문자로 구성된 문자열

```python
r = re.compile('[A-Z]+')
r.findall('HOME, home')
# 결과 : ['HOME']
```

3) a로 시작하고 세 글자로 구성된 문자열 

```python
r = re.compile('^a..')
r.findall('abc,case')
# 결과 : ['abc']
```

4) a, b가 각각 2번 이상 3번 이하 나타나는 패턴

```python
r = re.compile('a{2,3}b{2,3}')
r.findall('aabb, aaabb, ab, aab')
# 결과 : ['aabb', 'aaabb']
```

- 지정한 정규 표현식으로 추출 : group method

```python
p = re.compile('.+:')
m = p.search('http://google.com')
m.group()
# 결과 : 'http:'
```

- 지정한 정규 표현식과 일치하는 부분 교체 : sub method

```python
p = re.compile('(금|은|동)')
p.sub('금', '한국 선수가 동메달을 얻었다.')
# 결과 : '한국 선수가 금메달을 얻었다.'
```

## 사전 처리

1. 대소문자 통일 : `lower()` & `upper()`
2. 숫자, 문장부호, 특수문자 제거 : `re.compile('\W+')` (밑줄까지 삭제하는 경우 앞의 코드 실행 후, `re.compile('_')` )
3. 불용어 제거 : NLTK package의 `stopwords` library (한국어 지원 X)
4. 같은 어근 동일화(stemming) : NLTK package의 `PorterStemmer` library (+ `LancasterStemmer`, `RegexpStemmer`)
5. N-gram : NLTK package의 `ngrams`

```python
## 숫자, 문장부호, 특수문자 제거 예시
p = re.compile('\W+')
pp = p.sub(' ', '★축★ 한국 양궁 금메달 획득!_9연패 신화 달성')
p = re.compile('_')
p.sub(' ', pp)
# 결과 : ' 축 한국 양궁 금메달 획득  9연패 신화 달성'
```

```python
## stemming 예시
### PorterStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

ps_stemmer = PorterStemmer()
txt = "It is important to be immersed while you are pythoning with python. All pythoners have pythoned poorly at least once."
words = word_tokenize(txt)

for w in words:
    print(ps_stemmer.stem(w), end=' ')
# 결과 : it is import to be immers while you are python with python . all python have python poorli at least onc .

### LancasterStemmer
from nltk.stem.lancaster import LancasterStemmer
LS_stemmer = LancasterStemmer()

for w in words:
    print(LS_stemmer.stem(w), end=' ')
# 결과 : it is import to be immers whil you ar python with python . al python hav python poor at least ont .

### RegexpStemmer : 특정 표현식을 일괄적으로 제거 (위의 두 방식이 처리하지 못하는 부분에 적절하게 사용)
from nltk.stem.regexp import RegexpStemmer
RS_stemmer = RegexpStemmer('python')

for w in words:
    print(RS_stemmer.stem(w), end=' ')
# 결과 : It is important to be immersed while you are ing with  . All ers have ed poorly at least once .
```

```python
## N-gram : uni-gram과 bi-gram 이상의 n-gram을 혼합하여 단어 도출하는 것이 이상적
from nltk import ngrams
sentence = 'Cheif Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans and people of the world, thank you.'

grams = ngrams(sentence.split(), 2)

for gram in grams:
    print(gram, end=' ')
# 결과 : ('Cheif', 'Justice') ('Justice', 'Roberts,') ('Roberts,', 'President') ('President', 'Carter,') ('Carter,', 'President') ('President', 'Clinton,') ('Clinton,', 'President') ('President', 'Bush,') ('Bush,', 'President') ('President', 'Obama,') ('Obama,', 'fellow') ('fellow', 'Americans') ('Americans', 'and') ('and', 'people') ('people', 'of') ('of', 'the') ('the', 'world,') ('world,', 'thank') ('thank', 'you.')
```

### 품사 분석 (Part-Of-Speech : POS 태깅)

예시 :

1) 문서 안에 단어들을 추출하여 문서의 감정 정도 파악하는 경우 → 형용사 단어만 집중 추출

2) 동일한 단어가 다른 품사로 쓰인 경우 → 심층적 의미 발견

3) 품사별 단어의 빈도수 분석 → 형용사 비율이 높다면 문서에 주관적인 논조가 많다고 판단

- 한국어 품사 분석 : KoNLPy package (`Kkma`, `Komoran`, `Hannaum`, `Twitter`, `Mecab` class)

— 문자의 개수가 많아질수록 `Kkma` class의 실행 시간 ↑

— 단, `Kkma`의 경우 외래어, 외국어, 방언 등의 다양한 표현 모두 인지 가능 → 정확도 good

>> 분석에 따라 실행 시간과 정확도의 중요성에 따라 class 선택할 것

- 영어 품사 분석 : NLTK package의 `pos_tag()`

```python
import nltk

txt = "It is important to be immersed while you are pythoning with python. All pythoners have pythoned poorly at least once."

words = nltk.tokenize.word_tokenize(txt)
print(nltk.tag.pos_tag(words)) 
# 결과 : [('It', 'PRP'), ('is', 'VBZ'), ('important', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('immersed', 'VBN'), ('while', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('pythoning', 'VBG'), ('with', 'IN'), ('python', 'NN'), ('.', '.'), ('All', 'DT'), ('pythoners', 'NNS'), ('have', 'VBP'), ('pythoned', 'VBN'), ('poorly', 'RB'), ('at', 'IN'), ('least', 'JJS'), ('once', 'RB'), ('.', '.')]
```

![image 0](https://github.com/Ewha-BiE/study/blob/2.%EC%9E%A1%EC%95%84%EB%9D%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B9%80%EC%A3%BC%ED%98%84_image/image%200.png)

(출처 : [https://www.guru99.com/pos-tagging-chunking-nltk.html](https://www.guru99.com/pos-tagging-chunking-nltk.html))

## 텍스트 마이닝 기법

### 단어 빈도 분석 : 가장 기본적이면서 보편적으로 활용되는 방법

— 특정 단어의 출현 빈도가 높으면 핵심 단어로 간주

— 특정 핵심 단어를 선정 한 후, 해당 단어와 같은 문맥에서 출현한 단어들의 빈도를 분석하고 연도별로 비교할 것 (단, 불용어는 사전에 제거)

— 단어 빈도 분석 표현 방법 : 표, 단어구름(wordcloud) (특히 단어구름의 경우에는 문서별 비교, 대비할 때 문서 간 차이를 쉽게 확인할 수 있음) 


![image 1](https://github.com/Ewha-BiE/study/blob/2.%EC%9E%A1%EC%95%84%EB%9D%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B9%80%EC%A3%BC%ED%98%84_image/image%201.png)

단어 구름 예시 (출처 : [https://wordcloudapi.com/](https://wordcloudapi.com/)) 

```python
import pandas as pd
import nltk
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

## 단어 빈도 분석
f = open('./트럼프취임연설문.txt', 'r')
lines = f.readlines()[0]
f.close()

lines[0:100]
# 결과 : ' Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow'

tokenizer = RegexpTokenizer('[\w]+')
stop_words = stopwords.words('english')

words = lines.lower() # 모든 단어를 소문자로 변환
tokens = tokenizer.tokenize(words) # 단어 단위로 토큰화
stopped_tokens = [i for i in list((tokens)) if not i in stop_words] # 불용어 제거
stopped_tokens2 = [i for i in stopped_tokens if len(i)>1] # 한 글자 제거

pd.Series(stopped_tokens2).value_counts().head(10) # 최종 단어 리스트 반환하여 pandas Series 형태로 변환 후, 단어 빈도 카운트
# 결과 : 
## america     20
## american    11
## people      10
## country      9
## one          8
## nation       7
## every        7
## back         6
## new          6
## never        6
## dtype: int64
```

( 한국어 분석 : 한나눔 형태소 분석기 `from konlpy.tag import Hannanum` 사용 & 명사 추출이 가장 일반적)

```python
## 단어 구름 생성
from wordcloud import WordCloud
from collections import Counter

wordcloud = WordCloud(
    width = 800,
    height = 800,
    background_color = 'white'
)

count = Counter(stopped_tokens2)
wordcloud = wordcloud.generate_from_frequencies(count)
def __array__(self):
    """Convert to numpy array.
    Returns
    -------
    image : nd_array size (width, height, 3)
        Word cloud image as numpy matrix.
    """
    return self.to_array()

def to_array(self):
    """Convert to numpy array.
    Returns 
    -------
    image : nd-array size (width, height, 3)
        Word cloud image as numpy matrix.
    """
    return np.array(self.to_image())

array = wordcloud.to_array()

%matplotlib inline
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10, 10))
plt.imshow(array, interpolation='bilinear')
plt.show();
```

![image 2](https://github.com/Ewha-BiE/study/blob/2.%EC%9E%A1%EC%95%84%EB%9D%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B9%80%EC%A3%BC%ED%98%84_image/image%202.png)

```python
## 특성 형상에 맞춰 단어 구름 생성
###(원본 사진 출처 : https://electronicprogrammers.com/74)
from PIL import Image
import numpy as np

president = np.array(Image.open('./president.jfif'))

count = Counter(stopped_tokens2)
wc_pr = WordCloud(
    mask = president,
    background_color = 'white'
)
wc_pr = wc_pr.generate_from_frequencies(count)

plt.figure(figsize=(8, 8))
plt.imshow(wc_pr, interpolation='bilinear')
plt.axis('off')
plt.show();

```

![image 3](https://github.com/Ewha-BiE/study/blob/2.%EC%9E%A1%EC%95%84%EB%9D%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B9%80%EC%A3%BC%ED%98%84_image/image%203.png)

```python
## 실제 그림과 유사한 색상으로 단어 구름 생성
from wordcloud import ImageColorGenerator

image_colors = ImageColorGenerator(president)

plt.figure(figsize=(8, 8))
plt.imshow(wc_pr.recolor(color_func=image_colors), interpolation='bilinear')
plt.axis('off')
plt.show();
```

![image 4](https://github.com/Ewha-BiE/study/blob/2.%EC%9E%A1%EC%95%84%EB%9D%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B9%80%EC%A3%BC%ED%98%84_image/image%204.png)

### 유사한 데이터 간 군집 분석

- 분할 군집 분석

: 단순하고 보편적으로 사용

: 가정 - 데이터를 k개의 배타적인 집단으로 나눌 때, 군집의 숫자 k는 배경 지식을 통해 결정

(연구자의 주관 개입)

: k를 정하는 대표적 방법 - silhouette, elbow

— K-means clustering

: sample 간 거리 측정 방식 - 유클리디안 기하 거리, 코사인 거리 (= 1-코사인 유사도)

: TF-IDF와 같은 가중치가 각 문서의 벡터로 들어감 (주로 텍스트 마이닝에 이용, 여러 문서로 이루어진 문서군에서 특정 단어가 해당 문서에서의 중요도를 통계적으로 나타낸 수치)

: outlier에 민감 → (대안) K-대푯값 군집 분석  ** 단, 반복횟수가 많아져 시간 ↑

>> TF : term frequency (특정 단어가 문서 내에서 얼마나 자주 등장하는가) 

>> DF : document frequency (단어가 문서군 내에서 얼마나 자주 사용되는가)

 즉, $TF-IDF = TF*IDF = TF*1/DF$

```python
## K-Means
### 참고 : https://medium.com/@lucasdesa/text-clustering-with-k-means-a039d84a941b
kmeans = KMeans(n_clusters=5, 
                init = 'k-means++',
                n_init = 10,
                tol = 0.0001,
                random_state =42,
                algorithm = 'full').fit(final_df)

kmeans.labels_

# 결과 : array([4, 4, 1, 4, 2, 2, 1, 2, 2, 1, 2, 1, 0, 3, 0, 2, 4, 1, 1, 1, 4, 4,
#       2, 1, 1, 1, 3, 1, 1, 3, 3, 4, 0, 0, 2, 2, 3, 2, 2, 2, 1, 1, 2, 2,
#       0, 4, 1, 1, 4, 0, 3, 0, 1, 3, 4, 0, 2, 3, 0, 2, 4, 3, 0, 1, 2, 2,
#       1, 2, 1, 3, 2, 0, 3, 2, 3, 2, 3, 1, 2, 2])
```

```python
## K-medoids
from pyclustering.cluster import kmedoids

kmedoids_instance = kmedoids.kmedoids(final_df.values, initial_index_medoids=np.random.randint(42, size=5))
kmedoids_instance.process()
clusters = kmedoids_instance.get_clusters()
clusters
```

```python
## PCA (해당 데이터 셋에서는 PCA에 의한 차원 축소가 제대로 되지 않아 일부만 그래프상에 표현)
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(final_df)
principalDF = pd.DataFrame(data=principalComponents,
                           columns = ['principal component 1', 'principal component 2'])

plt.scatter(principalDF.iloc[kmeans.labels_ == 0, 0], principalDF.iloc[kmeans.labels_ ==0, 1], s = 10, c = 'red', label = 'cluster1')
#plt.scatter(principalDF.iloc[kmeans.labels_ == 1, 0], principalDF.iloc[kmeans.labels_ ==1, 1], s = 10, c = 'yellow', label = 'cluster2')
#plt.scatter(principalDF.iloc[kmeans.labels_ == 2, 0], principalDF.iloc[kmeans.labels_ ==2, 1], s = 10, c = 'green', label = 'cluster3')
plt.scatter(principalDF.iloc[kmeans.labels_ == 3, 0], principalDF.iloc[kmeans.labels_ ==3, 1], s = 10, c = 'blue', label = 'cluster4')
#plt.scatter(principalDF.iloc[kmeans.labels_ == 4, 0], principalDF.iloc[kmeans.labels_ ==4, 1], s = 10, c = 'black', label = 'cluster5')

plt.legend();
```

![image 5](https://github.com/Ewha-BiE/study/blob/2.%EC%9E%A1%EC%95%84%EB%9D%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B9%80%EC%A3%BC%ED%98%84_image/image%205.png)


- 구조적 군집 분석

: 데이터 오브젝트를 트리 형태의 군집으로 분할

: 개별 대상 간 거리를 통해 점차 대상을 결합해가면서 트리 모양의 계층 구조 형성 → 위계적 질서

: 데이터의 전체 구조를 한 눈에 파악 가능

— 거리 측정법 (군집 간 거리 측정 방식)

1) 최소 거리 : outlier & noise에 민감

2) 최대 거리 : outlier & noise에 민감

3) 평균 거리 : 간단한 계산

4) 거리 평균 : 정성적 데이터에도 적용 가능

5) Ward 측정법 ($\frac{n_A\cdot n_B}{n_A+n_B} ||\vec m_A-\vec m_B||^2$) : outlier & noise에 robust + 비슷한 크기의 군집끼리 묶어주는 경향성

```python
## Ward distance
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as shc

cluster = AgglomerativeClustering(n_clusters=5, linkage='ward')
cluster.fit_predict(final_df)
# 결과 : array([2, 2, 0, 2, 0, 2, 0, 0, 0, 3, 1, 0, 2, 0, 0, 0, 0, 3, 0, 1, 1, 2,
#       2, 2, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 1, 2, 2, 1, 0, 1,
#       4, 4, 0, 0, 0, 4, 0, 4, 4, 0, 4, 4, 1, 1, 4, 0, 4, 0, 4, 0, 4, 0,
#       0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0], dtype=int64)

plt.figure(figsize=(10, 7))
plt.title('Customer Dendograms')
dend = shc.dendrogram(shc.linkage(final_df, method='ward'))
```

![image 6](https://github.com/Ewha-BiE/study/blob/2.%EC%9E%A1%EC%95%84%EB%9D%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/%EA%B9%80%EC%A3%BC%ED%98%84_image/image%206.png)
