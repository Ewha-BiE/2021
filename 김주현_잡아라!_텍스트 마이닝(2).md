# 잡아라! 텍스트 마이닝 with 파이썬 (2)

출판사 : BJpublic

지은이 : 서대호

## 토픽 모델링

- 방대한 문헌에서 주제를 찾아내는 알고리즘

→ 단어가 쓰인 맥락을 고려하여 단어들을 클러스터링 → 주제 추론

ex) 뉴스, 블로그, SNS

### LDA (Latent Dirichlet Allocation)

: 토픽 모델링 기법 중 가장 많이 활용 (`genism` library 사용)

- 단어의 교환성 (단어의 순서보다 존재의 유무에 집중) 가정 有

```python
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim
from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer('[\w]+')
stop_words = stopwords.words('english')
p_stemmer = PorterStemmer()

doc_a = 'Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.'
doc_b = 'My mother spends a lot of time driving my brother around to baseball practice.'
doc_c = 'Some health experts suggest that driving may cause increased tension and blood pressure'
doc_d = 'I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.'
doc_e = 'Health professionals say that brocolli is good for your health.'
doc_f = 'Big data is a term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with.'
doc_g = 'Data with many cases offer greater statistical power, while data with higher complexity may lead to a higher false discovery rate.'
doc_h = 'Big data was originally associated with three key concepts : volume, variety, and velocity.'
doc_i = 'A 2016 definition states that "Big data represents the information assets characteried by such a high volume, velocity and variety to require specific technology and analytical methods for its transformation into value."'
doc_j = 'Data must be processed with advanced tools to reveal meaningful information.'

# 문서화
doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]

texts = []

for w in doc_set:
    raw = w.lower()
    tokens = tokenizer.tokenize(raw)
    stopped_tokens = [i for i in tokens if not i in stop_words]
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    texts.append(stemmed_tokens) ## 전처리 후, 문서별 단어 삽입

dictionary = corpora.Dictionary(texts) # 문서의 단어를 사전형으로 변경
corpus = [dictionary.doc2bow(text) for text in texts] # 문서-단어 matrix 형성

ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary)

## 단어 옆 숫자는 가중치 표현 (비중 나타냄)
ldamodel.print_topics(num_words=5)

ldamodel.get_document_topics(corpus)[0] # 0번째 문서의 토픽 1~3의 분포 (합하면 1)
```

 Q. LDA parameter 중 topic 수는 어떻게 결정하는가?

 A. 사전 정보가 있는 경우 그에 기반하여 지정, 사전 정보가 없으면 통계적 방법을 통해 토픽 개수 지정

- LDA 토픽 개수 지정 (perplexity or topic coherence 점수 사용)

 1) **perplexity** (혼란도) : 특정 확률 모델이 실제 관측 값을 얼마나 잘 예측하는지 평가

 → 토픽 모델링의 경우, 문헌 내 주제 출현 확률과 주제 내 용어 출현 확률을 계산한다는 점에서 확률 모델이므로 perplexity 사용 

— 토픽 개수가 증가할수록 perplexity는 감소하는 경향이 있으나 특정 토픽 개수를 기준으로 더 이상 perplexity가 수렴하게 됨 → 이때의 perplexity가 최종 perplexity이며 해당 값이 작을수록 토픽 모델은 실제 문화 결과를 잘 반영한다고 판단

2) **Topic coherence** : 사람이 해석하기에 적합한 평가 척도를 만들기 위해 제시된 척도

— 토픽 모델링의 결과로 나온 주제들에 대해 각 주제에서 상위 n개의 단어 뽑아냄 (모델링이 잘 되었다면 한 주제 안에는 의미가 유사한 단어들이 많이 모여있게 됨) → 상위 단어 간의 유사도의 평균을 계산하여 실제로 해당 주제가 의미론적으로 일치하는 단어끼리 모여있는지 판단  

```python
from gensim.models import CoherenceModel

print('\nPerplexity : ', ldamodel.log_perplexity(corpus))

coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, topn=10)
coherence_lda = coherence_model_lda.get_coherence()

print('\nCoherence Score : ', coherence_lda)
```

** 토픽의 개수를 다르게 하여 지표 계산 (시각화)

```python
import matplotlib.pyplot as plt

perplexity_values=[]
for i in range(2, 10):
    ldamodel=gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=dictionary)
    perplexity_values.append(ldamodel.log_perplexity(corpus))

x = range(2, 10)
plt.plot(x, perplexity_values)
plt.xlabel('Number of topics')
plt.ylabel('Perplexity score')
plt.show()
```

```python
coherence_values = []
for i in range(2, 10):
    ldamodel=gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=dictionary)
    coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, topn=10)
    coherence_lda = coherence_model_lda.get_coherence()
    coherence_values.append(cohrence_lda)
    
x - range(2, 10)
plt.plot(x, coherence_values)
plt.xlabel('Number of topics')
plt.ylabel('coherence score')
plt.show()
```

: 두 지표 모두 토픽 개수 3개 이후에서 값이 크게 변동하므로 최종 토픽 개수는 3으로 지정하는 것이 합리적이라고 판단

## 감성 분석

- 텍스트의 주관성 요소 탐지 → 긍정/부정 요소 및 정도성 판별 → 정량화 (글쓴이의 의도나 입장 분석 포함)

### 단어 사전 기반 분석

 : 어떤 단어가 어떤 감성을 전달하는 단어인지 알려줄 보조 자료 필수

 : 감성 사전을 이용하여 각 단어의 감정 분류와 정도를 알 수 있어야 함

- 긍정/부정 단어 시각화 : 단어 구름 or 단어 네트워크 (+ 토픽 모델링과의 결합, 연도/성별/인물별로 감성 정도와 감성 표현 분류)
- 영어 감성 사전 종류 : AFINN, EmoLex, Bing Liu lexicon, SentiWordNet

### 지도 기계 학습 기반 분석

: 훈련 데이터(입력 객체에 대한 속성을 백터 형태로 포함)로부터 하나의 함수를 유추하기 위한 기계 학습의 일종

: 훈련 데이터에서 지도 학습 모델링 → 모델을 이용해 주어진 데이터에 대해 예측 ⇒ 텍스트 마이닝 감성 분석에 적용 (전체 데이터 중 일부 문서를 훈련 데이터로 설정하여 인간이 긍정/부정 라벨링)

 ⇒ 따라서, 텍스트와 해당 텍스트의 각 문서별 감성 지수에 대한 사전 정의 & 텍스트의 주제, 장르, 맥락이 균일한 예측 모델에 적용할 같은 성격의 새로운 텍스트 필요

— 사전 기반 감성 분석에 비해 더 세밀하고 특수한 경우의 텍스트에 유용

— 단, 초기에 대량의 훈련 데이터에 레이블 포함되어야 함

- 기계 학습 모델 : 서포트 벡터 머신, 회귀 분석, 신경망, 나이브 베이즈 분류, 의사결정트리

## 연관어 분석

- 두 단어가 해당 문맥에서 얼마나 연관되어 있는가 → **시각화 중요**

    가장 간단한 방식 : 같은 문서에서 두 단어가 함께 출현하는 빈도 계산

    etc) 통계적으로 유사도 측정, 딥러닝 (word2vec) 유사도 

1) 대상어 선정 : 네트워크를 분석할 때 연구의 주된 대상으로 삼는 단어 (목적에 따라 결정)

ex) 관심 단어 혹은 고빈도 단어로 선정

2) 연관어 선정에서 어떤 문맥을 대상으로 추출할지 결정

ex) 문서-가장 보편적-, 문단, 문장, 문장 내 윈도우 범위의 단어

- 시각화 툴 : Gephi, Centriufuge, Commetrix

### 동시 출현 기반 연관어 분석

 : 같은 문맥에서 대상어와 다른 단어들의 동시 출현 빈도 계산하는 방법

- 가정

     : 해당 문맥에서 두 단어가 우연히 함께 나타날 빈도 < 실제로 함께 나타나는 빈도 

    ⇒ 강한 연관 관계 성립

따라서, 임계값 이상의 동시 출현 횟수를 갖는 대상어와 단어 간 페어만 남기고 나머지 필터링

(이때, 결과가 왜곡되지 않도록 문맥 내에서 여러 번 같은 단어가 발생하면 중복을 제외하고 계산하기도 함)

```python
import pandas as pd
import glob
from afinn import Afinn
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import numpy as np
import matplotlib.pyplot as plt

pos = (glob.glob('./*.tet'))[0:100]

lines_pos = []
for i in pos:
    try:
        f = open(i, 'r')
        temp = f.readlines()[0]
        lines_pos.append(temp)
        f.close()
    except Exception as e:
        continue

tokenizer = RegexpTokenizer('[\w]+')
stop_words = stopwords.words('english')

count= {} # 동시 출현 빈도 저장할 dict
for line in lines_pos:
    words = line.lower()
    tokens = tokenizer.tokenize(words)
                                    ## 동일 문맥 내 중복된 단어 -> 빈도 1로 취급
    stopped_tokens = [i for i in list(set(tokens)) if not i in stop_words+['br']]
    stopped_tokens2 = [i for i in stopped_tokens if len(i) > 1]
    for i ,a in enumerate(stopped_tokens2):
        for b in stopped_tokens2[i+1:]:
            if a>b :
                count[b, a] = count.get((b, a), 0) + 1
            else:
                count[a, b] = count.get((a, b), 0) + 1
                
df = pd.DataFrame.from_dict(count, orient='index')

list1 = []
for i in range(len(df)):
    list1.append([df.index[i][0], df.index[i][1], df[0][i]])
    
df2 = pd.DataFrame(list1, columns=['term1', 'term2', 'freq'])
df3 = df2.sort_values(by=['freq'], ascending=False)
df3 = df3.reset_index(drop=True)
```

### 통계적 가중치 기반 연관어 분석

- 단어 마다 가중치 할당하는 방식 : 출현 빈도, tf-idf, tf-icf
- 유사도 계산 방식 : cosine similarity, jaccard similarity, overlap similarity

```python
import pandas as pd
import glob
from afinn import Afinn
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse

pos = (glob.glob('./*.txt'))[0:100]

lines_pos = []
for i in pos :
    try:
        f = open(i, 'r')
        temp = f.readlines()[0]
        lines_pos.append(temp)
        f.close()
    except Exception as e:
        continue

stop_words = stopwords.words('english')
## tf-idf 가중치 할당
vec = TfidfVectorizer(stop_words=stop_words)
vector_lines_pos = vec.fit_transform(lines_pos)
A = vector_lineS_pos.toarray()
A = A.transpose() # 단어-문서 matrix로 전치
A_sparse = sparse.csr_matrix(A)

## 코시인 유사도 계산
similarities_sparse = cosine_similarity(A_sparse, dense_output=False)
ll = list(similarities_sparse.todok().items()) # 단어 페어가 인덱스 형태로 나타남

names = vec.get_feature_names() # 각 단어의 명칭에 접근

df = pd.DataFrame(ll, columns=['words', 'weight'])
df2 = df.sort_values(by=['weight'], ascending=False) # 내림차순 정렬
df2 = df2.reset_index(drop=True)
## 자기자신과의 페어는 무조건 1이 되므로 필터링
df3 = df2.loc[np.round(df2['weight']) < 1]
df3 = df3.reset_index(drop=True)
```

### word2vec 기반 연관어 분석

- 가정

1) 단어의 의미는 그 단어 주변 단어 분포로 이해될 수 있다.

2) 단어의 의미는 단어 벡터 안에 인코딩 될 수 있다.

- 위 방식과의 차이점 : 단순히 같은 문맥의 출현 횟수에 따라 가중치를 부여하는 방식이 아니라 단어 위치, 순서에 따라서 가중치 부여

— 가중치 산출 후에는 위의 방식과 동일하게 유사도 계산

- 방식

1) **CBOW** : 주변 단어로 중심 단어 예측하도록 모델 구축

2) **Skip-gram** : 중심 단어로 주변 단어 예측하도록 모델 구축, CBOW보다 정확도 높은 경우가 많음 (window size에 따라 반복학습)

```python
import pandas as pd 
import glob 
from nltk.corpus import stopwords 
from nltk.stem.porter import PorterStemmer
import numpy as np
from nltk.tokenize import RegexpTokenizer
from gensim.models.word2vec import Word2Vec

pos = (glob.glob('/*.txt'))[0:100]

lines_pos = []
for i in pos :
	try:
		f = open(i, 't')
		temp = f.readlines()[0]
		lines_pos.append(temp)
		f.close()
	except Exception as e:
		continue

stop_words = stopwords.words('english')
tokenizer = RegexpTokenizer('[\w]+')

text = []
for line in lines_pos:
	words = line.lower()
	tokens = tokenizer.tokenize(stopword)
	stopped_tokens = [i for i in list(set(tokens)) if not i in stop_words+['br']]
	stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]
	text.append(stopped_tokens2)

model = Word2Vec(text, sg=1, window=2, min_count=3) # word2vec model 
					#skip-gram / 중심 단어로부터 좌우 2개 단어까지 학습에 적용 / 전체 문서에서 최소 3번 이상 출현한 단어를 대상으로 학습 진행 
model.init_sims(replace=True)
# word1과 word2의 유사도 (default : cosine similarity)
model.wv.similarity('word1', 'word2')
# word3와 가장 유사한 단어 5개 (default : cosine similarity)
model.wv.most_similar('word3', topn=5)
```

** 단, 전체 데이터서 개별 단어의 상대적 중요성 알 수 없음

### 중심성(Centrality) 계수

- 각 단어별 중심성 계수로 중심성 계수 성격별 상대적 중요성 계산 가능 → 시각화할 때, 노드ㅣ 크기, 색상, 진하기 조절
- Python : `networkx` library

1) 연결 중심성 (degree centrality)

: 하나의 단어와 직접적으로 연결된 단어의 개수 측정 (국지적 범위 내에서의 역할)

2) 근접 중심성 (closeness centrality)

: 특정 단어와 연속적으로(직, 간접 모두 고려) 연결된 모든 단어와의 거리에 따른 평균 연관도 측정 (글로벌적 중요성 판단)

3) 매개 중심성 (betweenness centrality)

: 노드 간 링크를 타고 건너갈 때 핵심적으로 통과해야만 하는 노드 찾는 경우에 용이 → 매개 중심성이 클수록 네트워크 내 의사소통의 흐름에 영향 ↑ (단, 텍스트 마이닝에는 많이 활용 X)

4) 고유 벡터 중심성 (eigenvector centrality)

: 각 노드마다 중요성을 부과할 때 해당 노드와 연결된 노드들의 중심성 고려 → 높은 고유벡터 중심성을 가진 노드는 높은 점수를 가진 많은 노드와 연결됨을 의미 (연관 알고리즘 : PangeRank) 

```python
import pandas as pd 
import glob
from afinn import Afinn
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import numpy as np
import matplotlib.pyplot as plt

pos = (glob.glob('./*.txt'))[0:100]

lines_pos = []
for i in pos:
	try:
		f = open(i, 'r')
		temp = f.readlines()[0]
		lines_pos.append(temp)
		f.close()
	except Exception as e:
		continue

tokenizer = RegexpTokenizer('[\w]+')
stop_words = stopwords.words('english')

# 동시 출현 빈도 저장
count = {}
for line in lines_pos:
	words = line.lower()
	tokens = tokenizer.tokenize(words)
	stopped_tokens = [i for i in list(set(tokens)) if not i in stop_words+['br']]
	stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]
	for i, a in enumerate(stopped_tokens2):
		for b in stopped_tokens2[i+1]:
			if a>b:
				count[b, a] = count.get((b, a), 0) + 1
			else :
				count[a, b] = count.get((a, b), 0) + 1

df = pd.DataFrame.from_dict(count, orient='index')

list1 = []
for i in range(len(df)):
	list1.append([df.index[i][0], df.index[i][1], df[0][i]])

df2 = pd.DataFrame(list1, columns=['term1', 'term2', 'freq'])
df3 = df2.sort_values(by=['freq'], ascending=False)
df3_pos = df3.reset_index(drop=True)

# 중심성 계산
import networkx as nx
import operator

G_pos = nx.Graph()
									# 임계값 이상의 빈도 혹은 연관도를 지닌 단어 페어만 계산 
for i in rnage((len(np.where(df3_pos['freq'>10])[0]))):
	G_pos.add_edge(df3_pos['term1'][i], df3_pos['term2'][i], weight=int(df3_pos['freq'][i]))

dgr = nx.degree_centrality(G_pos)
btw = nx.betweenness_centrality(G_pos)
cls = nx.closness_centrality(G_pos)
egv = nx.eigenvector_centrality(G_pos)

sorted_dgr = sorted(dgr.items(), key=operator.itemgetter(1), reverse=True)
sorted_btw = sorted(btw.items(), key=operator.itemgetter(1), reverse=True)
sorted_cls = sorted(cls.items(), key=operator.itemgetter(1), reverse=True)
sorted_egv = sorted(egv.items(), key=opertaor.itmegetter(1), reverse=True)
```

### 연관어 네트워크 시각화

- Python code로 시각화할 때 여러 한계점 有 ⇒ **gephi** 시각화 툴 이용

1) only using Python

```python
for i in range(len(sorted_cls)):
	G.add_node(sorted_cls[i][0], nodesize=sorted_dgr[i][1])

for i in rnage((len(np.where(df3_pos['freq']>10)[0]))):
	G.add_weighted_edges_from([(df3_pos['term1'][i], df3_pos['term2'][i], int(df3_pos['freq'][i]))])

# 연결 중심성 계수로 노드의 크기 지정 
sizes = [G.node[node]['nodesize']*500 for node in G]

options = {
	'edge_color':'#FFDEA2',
	'width':1,
	'with_labels':True,
	'font_weigth':'regular'
}

nx.draw(G, node_size=sizes, pos=nx.spring_layout(G, k=3.5, iterations=50), **options)

ax = plt.gca()
ax.collections[0].set_edgecolor('#555555')
plt.show()
```

2) using gephi tool 

```python
# 파이썬에서 연관도 산출한 후 xml 형태로 출력하여 gephi에서 시각화
## https://gephi.org/users/ --> tutorial
class MakeGraphml:
	def make_graphml(self, pair_file, grahml_file):
		out = open(grahml_file, 'w', encoding='utf-8')
		entity = []
		e_dict = {}
		count = []
		for i in range(len(pair_file)):
			e1 = pair_file.iloc[i, 0]
			e2 = pair_file.iloc[i, 1]
			# frq = ((word_dict[e1], word_dict[e2]), pair.split('\t')[2])
			frq = ((e1, e2), pair_file.iloc[i, 2])
			if frq not in count: count.append(frq) ## ((a, b), frq)
			if e1 not in entity: entity.appned(e1)
			if e2 not in entity: entity.append(e2)
		print('# terms: %s'% len(entity))

		for i, w in enumerate(entity):
			e_dict[w] = i+1 # {word : id}

		out.write(
			"<?xml version=\"1.0\" encoding=\"UTF-8\"?><graphml xmlns=\"http://graphml.graphdrawing.org/xmlns\"xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"xsi:schemaLocation=\"http://graphml.graphdrawing.org/xmlnshttp://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd\">"
			+
			"<key id=\"d1\" for=\"edge\" attr.name=\"weight\" attr.type=\"double\"/>" +
			"<key id=\"d0\" for=\"node\" attr.name=\"label\" atrr.type=\"string\"/>" +
			"<graph id=\"Entity\" edgedefault=\"undirected\">" + "\n"
			)
			
		# nodes
		for i in entity:
			out.write("<node id=\"" + str(e_dict[i]) + "\">" + "\n")
			out.write("<data key=\"d0\">" + i + "</data>" + "\n")
			out.write("</node>")

		# edges
		for y in range(len(count)):
			out.write("<edge source=\"" + str(e_dict[count[y][0][0]]) + "\"target=\"" + str(e_dict[count[y][0][1]]) + "\">" +"\n")
			out.write("<data key=\"d1\">" + str(count[y][1]) + "</data>" + "\n")
			out.write("</edge>")

		out.write("</graph> </graphml>")
		print('now you can see %s' % graphml_file)

		out.close()

gm = MakeGraphml()
				# 파일이 만들어질 경로와 이름 지정
graphml_file = 'network.graphml'
									# 동시 출현 빈도가 10보가 큰 행만 입력 
gm.make_graphml(df3_pos.iloc[0:(len(np.where(df3_pos['freq']>10)[0])), :], graphml_file)
```